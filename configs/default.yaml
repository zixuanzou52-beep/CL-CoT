# CL-CoT Default Configuration

# Model configuration
model:
  base_model: "meta-llama/Llama-2-13b-hf"
  use_lora: true
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# Encoder configuration
encoder:
  hidden_dim: 768
  num_layers: 6
  num_heads: 12
  dropout: 0.1
  max_position_embeddings: 512

# Contrastive learning configuration
contrastive:
  temperature: 0.07
  memory_bank_size: 10000
  momentum: 0.999
  negative_ratio: 5  # negative:positive ratio
  hard_negative_ratio: 0.4
  soft_negative_ratio: 0.4
  adversarial_negative_ratio: 0.2

# Reinforcement learning configuration
rl:
  gamma: 0.95  # discount factor
  ppo_epsilon: 0.2  # clipping parameter
  value_coef: 0.5
  entropy_coef: 0.01
  num_episodes: 1000
  ppo_epochs: 4
  gae_lambda: 0.95

# Reward function configuration
reward:
  penalty_coef: 0.5  # penalty for incorrect answers
  eff_weight: 0.3    # efficiency weight
  int_weight: 0.2    # interpretability weight
  max_steps: 15

# Similarity configuration
similarity:
  struct_weight: 0.3
  semantic_weight: 0.5
  op_weight: 0.2

# Training configuration
training:
  # Stage 1 - Supervised
  stage1_lr: 2.0e-5
  stage1_epochs: 3
  stage1_batch_size: 32
  stage1_warmup_steps: 500

  # Stage 2 - Contrastive
  stage2_lr: 1.0e-5
  stage2_epochs: 5
  stage2_batch_size: 64
  stage2_warmup_steps: 300

  # Stage 3 - RL
  stage3_lr: 3.0e-6
  stage3_batch_size: 16
  stage3_warmup_steps: 100

  # Common
  max_grad_norm: 1.0
  weight_decay: 0.01
  logging_steps: 100
  eval_steps: 500
  save_steps: 1000
  gradient_accumulation_steps: 2
  fp16: true
  max_length: 512

# Inference configuration
inference:
  max_generation_length: 512
  num_beams: 1
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  max_reasoning_steps: 15
  num_path_candidates: 5

# Data configuration
data:
  max_table_rows: 50
  max_table_cols: 20
  max_question_length: 128
  num_workers: 4
  cache_dir: "data/cache"

# Hardware configuration
hardware:
  gpus: 8
  mixed_precision: "fp16"
  deepspeed_config: null

# Paths
paths:
  data_dir: "data/processed"
  checkpoint_dir: "experiments"
  log_dir: "logs"
  result_dir: "results"

# Experiment tracking
wandb:
  project: "CL-CoT"
  entity: null
  enabled: true
  log_model: false

# Random seed
seed: 42
